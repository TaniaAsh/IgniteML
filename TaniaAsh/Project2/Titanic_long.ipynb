{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All libraries we need\n",
    "[link 1](https://eshitagoel.medium.com/eda-on-titanic-machine-learning-from-disaster-6b518bb97e17)\n",
    "[link 2](https://medium.com/geekculture/applying-7-classification-algorithms-on-the-titanic-dataset-278ef222b53c)\n",
    "[link 3](https://www.kaggle.com/code/tarekmuhammed/classification-prj-classify-survived-or-not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "path_to_data = 'Titanic-Dataset.csv'\n",
    "df = pd.read_csv(path_to_data)\n",
    "print(df.columns)  # Inspect column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # Показать первые 5 строк (по умолчанию) или указать количество строк, например: df.head(10)\n",
    "#df.tail() \n",
    "#df.shape # rows, columns\n",
    "#df.describe()\n",
    "#df['currentEnergyRating'].nunique() \n",
    "#df.iloc[-5:, :]\n",
    "#df.iloc[2:5,1:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 889 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  889 non-null    int64  \n",
      " 1   Pclass    889 non-null    int64  \n",
      " 2   Age       889 non-null    float64\n",
      " 3   SibSp     889 non-null    int64  \n",
      " 4   Parch     889 non-null    int64  \n",
      " 5   Fare      889 non-null    float64\n",
      " 6   Embarked  889 non-null    int64  \n",
      " 7   female    889 non-null    int64  \n",
      " 8   male      889 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 69.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin is the most columns has NAN Values \n",
    "df.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in df.columns]\n",
    "\n",
    "for col in columns:\n",
    "    print(\"\\n\",df[col].value_counts(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "for col in columns:\n",
    "    print(\"\\n\",df[col].value_counts(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.title('Pairplot of Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = [\"#388087\"]\n",
    "\n",
    "# To show the outliers of each column\n",
    "for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.set_title(f'Boxplot of {col}')\n",
    "    bp = sns.boxplot(data=df, y=col, ax=ax, color=custom_palette[0])  # Use 'color' for single color\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='Age', kde=True, bins=30, color='green')\n",
    "plt.title('Distribution Plot of Age of Passengers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(hue='Sex', y='Survived', data=df, palette=custom_palette)\n",
    "plt.title('Violinplot of a gender')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='Sex', y='Age', data=df, hue='Sex', palette=custom_palette)\n",
    "plt.title('Violinplot of Gender')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='Fare', kde=True, bins=80, color='green')\n",
    "plt.title('Distribution Plot of Fare of Passengers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = [ \"#66CDAA\"]  \n",
    "sns.set_palette(custom_palette)\n",
    "sns.displot(data=df, x=\"Embarked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"Sex\",y=\"Pclass\", hue=\"Sex\", palette=\"viridis\",s=250)\n",
    "plt.xlabel('Pclass')\n",
    "plt.ylabel('Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"Embarked\",y=\"Pclass\", hue=\"Sex\", palette=\"viridis\",s=550)\n",
    "plt.xlabel('Pclass')\n",
    "plt.ylabel('Embarked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Operations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unuseful columns :\n",
    "df.drop(columns=[\"Name\",\"PassengerId\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Cabin.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns with big amount of missed data and null values \n",
    "df.drop(columns=[\"Cabin\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missed data with the mean in the age column \n",
    "df[\"Age\"] = df[\"Age\"].fillna(np.mean(df[\"Age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the records with missed data ; because there are only 2 records\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know every column will be encoded and how will be encoded\n",
    "columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "for col in columns:\n",
    "    print(\"\\n\",df[col].value_counts(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Ticket column because it has many values so it will be so hard to encode each value with label\n",
    "df.drop(\"Ticket\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummies encoding\n",
    "Get_Dumm=[\"Sex\"]\n",
    "\n",
    "for col in Get_Dumm :\n",
    "    Encoded_with_getdummies=pd.get_dummies(df[col],drop_first=False,dtype=\"int\")\n",
    "    df=pd.concat([df,Encoded_with_getdummies],axis=1)\n",
    "    df.drop(col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding \n",
    "Label_categ=[\"Embarked\"]\n",
    "for col in Label_categ :\n",
    "    df[col]=LabelEncoder().fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Data=MinMaxScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Data=pd.DataFrame(Scaled_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Data.columns=[\"Survived\",\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"female\",\"male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into features and Goal Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=Scaled_Data.drop(columns=\"Survived\")\n",
    "Goal=Scaled_Data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Groups into train and test groups :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train,Features_test,Goal_train,Goal_test=train_test_split(Features,Goal,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Grid Search Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_params={\"penalty\":['l1', 'l2','elasticnet', 'None'],\n",
    "           \"dual\":[True,False],\n",
    "           \"tol\":[1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1e-0,1e+1,1e+2,1e+3,1e+4,1e+5],\n",
    "           \"solver\":['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']}\n",
    "\n",
    "KNN_params={\"n_neighbors\":[3,4,5,6,7,8,9,10],\n",
    "            \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\":[\"uniform\",\"distance\"],\n",
    "            \"leaf_size\":[10,20,30,40,50]}\n",
    "\n",
    "DTC_params={\"max_leaf_nodes\":[1,3,4,5,7,8,\"None\"],\n",
    "            \"random_state\":[0,10,15,30,40,42,44,46,50,60],\n",
    "            'max_depth' : [3,5,7,10],\n",
    "            'criterion': ['gini', 'entropy']}\n",
    "\n",
    "RFC_params={\"n_estimators\":[100,120,160,200,240],\n",
    "            \"criterion\":['gini', 'entropy', 'log_loss'],\n",
    "            \"max_features\":[\"sqrt\",\"log2\",\"None\"],\n",
    "            \"bootstrap\":[True,False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Grid Search on Logistic Regression model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid with solver-penalty compatibility\n",
    "LR_params = {\n",
    "    'penalty': ['l1', 'l2'],  # Include only valid penalties for respective solvers\n",
    "    'solver': ['saga', 'liblinear'],  # These solvers support 'l1'\n",
    "    'tol': [0.1, 0.01],\n",
    "    'C': [1, 10, 100]\n",
    "}\n",
    "\n",
    "LR = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "GRS = GridSearchCV(LR, LR_params, cv=5, error_score='raise')  # Debugging with 'raise'\n",
    "GRS.fit(Features_train, Goal_train)\n",
    "\n",
    "print(GRS.best_params_)\n",
    "print(GRS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=LogisticRegression(dual=False,penalty=\"l2\",solver=\"saga\",tol=0.1)\n",
    "LR.fit(Features_train,Goal_train)\n",
    "LR_Pred=LR.predict(Features_test)\n",
    "LR_Pred_Train=LR.predict(Features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for the test group : \\n\")\n",
    "print(classification_report(Goal_test,LR_Pred),\"\\n\\n\")\n",
    "print(\"Classification Report for the train group (to check if there is any overfitting): \\n\")\n",
    "print(classification_report(Goal_train,LR_Pred_Train),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_RFC=confusion_matrix(Goal_test,LR_Pred)\n",
    "print(CM_RFC)\n",
    "sns.heatmap(CM_RFC,annot=True,cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Grid Search on KNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN=KNeighborsClassifier()\n",
    "GRS = GridSearchCV(KNN, KNN_params, cv = 5)\n",
    "GRS.fit(Features_train, Goal_train)\n",
    "\n",
    "print(GRS.best_params_)\n",
    "print(GRS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN=KNeighborsClassifier(algorithm=\"auto\",leaf_size=10,n_neighbors=8,weights=\"uniform\")\n",
    "KNN.fit(Features_train,Goal_train)\n",
    "KNN_Pred=KNN.predict(Features_test)\n",
    "KNN_Pred_Train=KNN.predict(Features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for the test group : \\n\")\n",
    "print(classification_report(Goal_test,KNN_Pred),\"\\n\\n\")\n",
    "print(\"Classification Report for the train group (to check if there is any overfitting): \\n\")\n",
    "print(classification_report(Goal_train,KNN_Pred_Train),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_KNN=confusion_matrix(Goal_test,KNN_Pred)\n",
    "print(CM_KNN)\n",
    "sns.heatmap(CM_KNN,annot=True,cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Grid Search on Decision Tree model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "DTC_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'max_leaf_nodes': [None, 5, 10, 20],  # Avoid conflicts with max_depth\n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "# Initialize the DecisionTreeClassifier\n",
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "# Apply GridSearchCV\n",
    "GRS = GridSearchCV(DTC, DTC_params, cv=5, error_score='raise')  # Debugging mode\n",
    "GRS.fit(Features_train, Goal_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(GRS.best_params_)\n",
    "print(GRS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=4, min_samples_split=2)\n",
    "DTC.fit(Features_train, Goal_train)\n",
    "DTC_Pred = DTC.predict(Features_test)\n",
    "DTC_Pred_Train = DTC.predict(Features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for the test group : \\n\")\n",
    "print(classification_report(Goal_test,DTC_Pred),\"\\n\\n\")\n",
    "print(\"Classification Report for the train group (to check if there is any overfitting): \\n\")\n",
    "print(classification_report(Goal_train,DTC_Pred_Train),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_DTC=confusion_matrix(Goal_test,DTC_Pred)\n",
    "print(CM_DTC)\n",
    "sns.heatmap(CM_DTC,annot=True,cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Grid Search on Random Forest model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "RFC_params = {\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 20, None],  # Depth of the tree\n",
    "    'max_features': ['sqrt', 'log2', None],  # Ensure valid values for max_features\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "# Apply GridSearchCV\n",
    "GRS = GridSearchCV(RFC, RFC_params, cv=5, error_score='raise')  # Debugging mode\n",
    "GRS.fit(Features_train, Goal_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(GRS.best_params_)\n",
    "print(GRS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC=RandomForestClassifier(bootstrap= True,class_weight='balanced_subsample', criterion=\"log_loss\" ,max_features=\"log2\",n_estimators= 120)\n",
    "RFC.fit(Features_train,Goal_train)\n",
    "RFC_Pred=RFC.predict(Features_test)\n",
    "RFC_Pred_Train=RFC.predict(Features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for the test group : \\n\")\n",
    "print(classification_report(Goal_test,RFC_Pred),\"\\n\\n\")\n",
    "print(\"Classification Report for the train group (to check if there is any overfitting): \\n\")\n",
    "print(classification_report(Goal_train,RFC_Pred_Train),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_RFC=confusion_matrix(Goal_test,RFC_Pred)\n",
    "print(CM_RFC)\n",
    "sns.heatmap(CM_RFC,annot=True,cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(RFC.feature_importances_,\n",
    "                          index = Features_train.columns).sort_values(ascending = False)\n",
    "\n",
    "sns.barplot(x = feature_scores, y = feature_scores.index)\n",
    "plt.xlabel(\"Feature Importances\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ftrain_RFC = Features_train.drop(columns=[\"Embarked\", \"Parch\"], axis=1)\n",
    "\n",
    "# Инициализация модели\n",
    "RFC_ii = RandomForestClassifier()\n",
    "\n",
    "# Обновленные параметры\n",
    "RFC_Params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400, 500, 600, 700, 800],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]  # None без кавычек\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "GRS = GridSearchCV(RFC_ii, RFC_Params, cv=5, error_score='raise')  # Debugging mode включен\n",
    "GRS.fit(Ftrain_RFC, Goal_train)\n",
    "\n",
    "# Вывод лучших параметров и оценки\n",
    "print(\"Лучшие параметры:\", GRS.best_params_, \"\\n\")\n",
    "print(\"Лучшая оценка:\", GRS.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка данных\n",
    "Ftest_RFC = Features_test.drop(columns=[\"Embarked\", \"Parch\"], axis=1)\n",
    "\n",
    "# Инициализация и обучение модели\n",
    "RFC_ii = RandomForestClassifier(bootstrap=True, class_weight=\"balanced\", criterion=\"gini\")\n",
    "RFC_ii.fit(Ftrain_RFC, Goal_train)\n",
    "\n",
    "# Предсказания\n",
    "RFC_Pred = RFC_ii.predict(Ftest_RFC)\n",
    "RFC_Pred_Train = RFC_ii.predict(Ftrain_RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for the test group : \\n\")\n",
    "print(classification_report(Goal_test,RFC_Pred),\"\\n\\n\")\n",
    "print(\"Classification Report for the train group (to check if there is any overfitting): \\n\")\n",
    "print(classification_report(Goal_train,RFC_Pred_Train),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_RFC=confusion_matrix(Goal_test,RFC_Pred)\n",
    "print(CM_RFC)\n",
    "sns.heatmap(CM_RFC,annot=True,cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the final Model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Based on the evaluation of various models, the Random Forest model, trained on the entire feature set, emerges as the \n",
    "most effective. It demonstrates a remarkable accuracy of 98% in predicting outcomes within the training dataset. \n",
    "Accompanying this high accuracy is an F1 score of 98%, reflecting the model's precision and recall performance metrics.Upon \n",
    "application to the testing dataset, the Random Forest model maintains a commendable accuracy rate of 78%, indicating its \n",
    "robustness in generalizing to unseen data. A meticulous examination of the confusion matrix further underscores the model's efficacy, as evidenced by the notable proportion of correctly predicted instances.\n",
    "Specifically, the Random Forest model accurately predicts 69 out of 89 instances, yielding an impressive true positive rate. \n",
    "Conversely, only 20 instances are misclassified, reaffirming the model's overall proficiency in classification tasks.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Encoded And Scaled Data as CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Encoded_titanic_data.csv\")\n",
    "Scaled_Data.to_csv(\"Scaled_titanic_data.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(RFC,\"Titanic Classifier Model (IEE Task).sav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
